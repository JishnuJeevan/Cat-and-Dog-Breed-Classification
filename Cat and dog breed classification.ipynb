{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>6. Cat and dog - Breed classification</center></h1>\n",
    "\n",
    "<p style='text-align: center;'> \n",
    "Jishnu Jeevan <br>\n",
    "Department of Computer Science <br>\n",
    "M.Tech Computer and Information Science <br>\n",
    "jishnujeevan@cusat.ac.in <br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center> Assignemt Objective</center></h2>\n",
    "<p style='text-align: justify;'>\n",
    "The data set contains pictures of cats and dog. The cat and dog belong to different breeds. Our objective is to classify the cat and dog to there repective breed. There are a total of 37 breeds, each having 200 pictures for each breed.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The data set contains the following information: </h2>\n",
    "<p style='text-align: justify;'>\n",
    "The following annotations are available for every image in the dataset: \n",
    "1. Species and breed name <br>\n",
    "2. A tight bounding box (ROI) around the head of the animal <br>\n",
    "3. A pixel level foreground-background segmentation (Trimap) <br>\n",
    "\n",
    "The cat breeds and the total number of pictures for each breeds are:<br>\n",
    "1. Abyssinian: 198 <br>\n",
    "2. Bengal: 200 <br>\n",
    "3. Birman: 200 <br>\n",
    "4. Bombay: 184 <br>\n",
    "5. British: 200 <br>\n",
    "6. Egyptian: 190 <br>\n",
    "7. Maine Coon: 200 <br>\n",
    "8. Persian: 200 <br>\n",
    "9. Ragdoll: 200 <br>\n",
    "10. Russian Blue: 200 <br>\n",
    "11. Siamese: 199 <br>\n",
    "12. Sphynx: 200 <br>\n",
    "\n",
    "The dog breeds and the toal number of pictures for each breeds are: <br>\n",
    "1. American Bulldog: 200 \n",
    "2. American Pit Bull Terrier: 200 \n",
    "3. Basset Hound: 200 \n",
    "4. Beagle: 200 \n",
    "5. Boxer: 199 \n",
    "6. Chihuahua: 200 \n",
    "7. English Cocker Spaniel: 196\n",
    "8. English Setter: 200\n",
    "9. German Shorthaired: 200\n",
    "10. Great Pyrenees: 200\n",
    "11. Havanese: 200\n",
    "12. Japanese Chin: 200\n",
    "13. Keeshond: 199\n",
    "14. Leonberger: 200\n",
    "15. Miniature Pinscher: 200\n",
    "16. Newfoundland: 196\n",
    "17. Pomeranian: 200\n",
    "18. Pug: 200\n",
    "19. Saint Bernard: 200\n",
    "20. Samoyed: 200\n",
    "21. Scottish Terrier: 199\n",
    "22. Shiba Inu: 200\n",
    "23. Staffordshire Bull Terrier: 189\n",
    "24. Wheaten Terrier: 200\n",
    "25. Yorkshire Terrier: 200\n",
    "\n",
    "The total numer of cats and dogs are: <br>\n",
    "1. Cats: 2371\n",
    "2. Dog: 4978\n",
    "3. Total: 7349\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For image procssing\n",
    "import cv2\n",
    "\n",
    "# For matrix operation\n",
    "import numpy as np\n",
    "\n",
    "# For acessing files from path\n",
    "import os\n",
    "\n",
    "# For random and shuffle operations\n",
    "from random import shuffle\n",
    "\n",
    "# To show a progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For checking types\n",
    "import types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Let us set the path of the image directory, the size of the images (as all images are not of the same size), learning rate and the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = './images'\n",
    "\n",
    "# Every image we have we are going to reshape it to 50 x 50\n",
    "IMG_SIZE = 50\n",
    "\n",
    "# Learning rate is 0.001\n",
    "LR = 0.001\n",
    "\n",
    "# Make a model name so we can save it and load it later\n",
    "MODEL_NAME = \"dog vs cat.model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a. We are going to convert the labels into one hot encoding, maually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lable of the image is of the form Abyssinian_1.jpg\n",
    "def label_img(img):\n",
    "    # Split the word by '.' and take the first word\n",
    "    word_label = img.split('.')[0]\n",
    "    \n",
    "    # Create a list containig all the breeds\n",
    "    breed_list =[\n",
    "                    \"Abyssinian_\", \"Bengal_\", \"Birman_\", \"Bombay_\", \"British_Shorthair_\", \"Egyptian_Mau_\",\n",
    "                    \"Maine_Coon_\", \"Persian_\", \"Ragdoll_\", \"Russian_Blue_\", \"Siamese_\", \"Sphynx_\",\n",
    "                    \"american_bulldog_\",\"american_pit_bull_terrier_\",\"basset_hound_\",\"beagle_\",\"boxer_\",\n",
    "                    \"chihuahua_\",\"english_cocker_spaniel_\",\"english_setter_\",\"german_shorthaired_\",\"great_pyrenees_\",\n",
    "                    \"havanese_\",\"japanese_chin_\",\"keeshond_\",\"leonberger_\",\"miniature_pinscher_\",\n",
    "                    \"newfoundland_\",\"pomeranian_\",\"pug_\",\"saint_bernard_\",\"samoyed_\",\n",
    "                    \"scottish_terrier_\",\"shiba_inu_\",\"staffordshire_bull_terrier_\",\"wheaten_terrier_\",\"yorkshire_terrier_\"\n",
    "                ]\n",
    "    \n",
    "    # iterate through the list\n",
    "    for index, breed in enumerate(breed_list):\n",
    "        # Check if the breed is in lable\n",
    "        if breed in word_label:   \n",
    "        \n",
    "            # create a list with 37 0's\n",
    "            one_hot = [0 for i in range(37)]\n",
    "\n",
    "            # Set the appropriate index to 1, for that breed\n",
    "            one_hot[index] = 1\n",
    "\n",
    "            # Retur the one_hot encoded variable \n",
    "            return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. We are going to convert the image file into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    # Create a list to store the image and its label\n",
    "    data = []\n",
    "    \n",
    "    # Let us check how many images are valid images\n",
    "    # Total images in directory\n",
    "    total_img = 0\n",
    "    \n",
    "    # Total valid images in directory\n",
    "    valid_img = 0\n",
    "    \n",
    "    # Take every image in the directory\n",
    "    for img in tqdm(os.listdir(DIR)):\n",
    "        \n",
    "        # Increment the number of images in directory\n",
    "        total_img = total_img + 1\n",
    "        \n",
    "        # Get the label of the image\n",
    "        label = label_img(img)\n",
    "        \n",
    "        # Find the full path of the image\n",
    "        path = DIR + \"/\" + img\n",
    "        \n",
    "        # Convert it to grey scale\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Chek if the image is a valid imaage file\n",
    "        if type(img) is np.ndarray:\n",
    "            \n",
    "            # Resize the image\n",
    "            img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        \n",
    "            # Append the image array and label\n",
    "            data.append([np.array(img), np.array(label)])\n",
    "            \n",
    "            # Increment the number of valid images\n",
    "            valid_img = valid_img + 1 \n",
    "    \n",
    "    # Shuffle the data set\n",
    "    shuffle(data)\n",
    "    \n",
    "    # Save it as an npy file\n",
    "    np.save('Data.npy', data)\n",
    "    \n",
    "    print(\"Total images in directory : \", total_img)\n",
    "    print(\"Valid images in directory : \", valid_img)\n",
    "    \n",
    "    # Return the data set\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loading the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. If you don't have an .npy file then create it by running this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7390/7390 [04:45<00:00, 25.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in directory :  7390\n",
      "Valid images in directory :  7384\n"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. If you do have a .npy file, load it using this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('Data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. I am going to use a neural network to identify the images. It will have 5 convolutional layer, a fully connected layer and an output layer. It will run a 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\helpers\\summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\helpers\\trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\layers\\core.py:81: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\initializations.py:119: calling UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:507: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\layers\\conv.py:552: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\initializations.py:174: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\layers\\core.py:239: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\optimizers.py:238: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\summaries.py:46: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tflearn\\helpers\\trainer.py:134: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To surpress warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reset the graph instance\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# We are going use tflearn to create the network\n",
    "import tflearn\n",
    "\n",
    "# For convolutional and max pooling layer\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "\n",
    "# For input layer and fully connected layer\n",
    "from tflearn.layers.core import input_data, fully_connected, dropout\n",
    "\n",
    "# For regression\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "# Input node\n",
    "convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "\n",
    "# First convolutional layer\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "# Second convolutional layer\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "# Third convolutional layer\n",
    "convnet = conv_2d(convnet, 128, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "# Fourth convolutional layer\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "# Fifth convolutional layer\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "# Fully connected layer\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "# Output layer\n",
    "convnet = fully_connected(convnet, 37, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_dir='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Let us now train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a. If the model already exist, load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\LENOVO\\Desktop\\M.Tech\\S1\\2. MLA\\Assaingment\\Assignment 6\\dog vs cat.model\n",
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "# If the model alreaady exist\n",
    "if os.path.exists('{}.meta'.format(MODEL_NAME)):\n",
    "    model.load(MODEL_NAME)\n",
    "    print('model loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6b. Let us split the data set to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 10799  | total loss: 0.53980 | time: 17.509s\n",
      "| Adam | epoch: 100 | loss: 0.53980 - acc: 0.8246 -- iter: 6848/6884\n",
      "Training Step: 10800  | total loss: 0.54697 | time: 18.680s\n",
      "| Adam | epoch: 100 | loss: 0.54697 - acc: 0.8219 | val_loss: 8.91028 - val_acc: 0.1480 -- iter: 6884/6884\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Take all but last 500 for training\n",
    "train = dataset[:-500]\n",
    "\n",
    "# Take last 500 for testing\n",
    "test = dataset[-500:]\n",
    "\n",
    "# This is the image we are going to sue for training\n",
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "# This is the label we are going to use for training\n",
    "Y = [i[1] for i in train]\n",
    "\n",
    "# This is the image we are going to use for testing\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "\n",
    "# This is the label we are going to use for testing\n",
    "test_y = [i[1] for i in test]\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X,Y, n_epoch=100, validation_set=(test_x, test_y),snapshot_step=500, show_metric=True, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6c. If we are happy with the model save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:C:\\Users\\LENOVO\\Desktop\\M.Tech\\S1\\2. MLA\\Assaingment\\Assignment 6\\dog vs cat.model is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model.save(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. As we can see we get an accuracy of 82%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
